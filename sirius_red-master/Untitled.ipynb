{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7181f6e-8bc4-4f12-b81f-dbf01db866da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference only\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "from Reinhard import Normalizer\n",
    "from data_augment import RandomAugment\n",
    "from dataset_generic import CustomDataset, load_dataframe, get_indep_test_sets, MultipleInferenceDataset\n",
    "from model_zoo import Encoder, get_ozan_ciga, freeze_encoder\n",
    "from ssc_utils import SSCMaxPoolClassifier, train_model_w_ssc, SSCGatedAttentionClassifier, get_scores_ssc, \\\n",
    "    get_scores_ssc_multiple_inference, train_model_w_ssc_multiple_inference\n",
    "from training import train_model, train_model_multiple_inference\n",
    "from utils import get_scores, set_device_and_seed, load_model_weights, get_scores_multiple_inference, get_model, get_model_ssc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9e932cd-f252-4e78-97f2-379ae65b1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    # Training Parameters\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size\": 6,\n",
    "    \"gpu_name\": \"cuda:0\",\n",
    "    \"seed\": 42,\n",
    "    \"multi_gpu\": False, # if True, use all available JPUs with DataParallel\n",
    "\n",
    "    # DataLoading parameters\n",
    "    \"tile_path\":\"/data/MSc_students_accounts/sneha/tiles_summary_5x_2.5mm_50%_adaptive_threshold.csv\", # abs path to csv file with tiling info\n",
    "    \"data_dir\": \"/data/goldin/images_raw/nafld_liver_biopsies_sirius_red/\", # abs path to loc of .ndpi files\n",
    "    \"multiple_inference\": False, # test with multiple inference dataset\n",
    "    \"bag_size\": 10, # tiles per bag\n",
    "\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    \"resize\": True, # resize images before trainnig\n",
    "    \"img_size\": 224, # resize to img_size x img_size patches\n",
    "    \"stain_normalization\": # apply reinhard stain normalisation based on source image in source_dir\n",
    "    {\n",
    "        \"apply_reinhard\": True, \n",
    "        \"source_dir\":\"/data/MSc_students_accounts/sneha/sneha/sirius_red-master/reinhard_source.jpg\"\n",
    "    },\n",
    "    \"transform_color_jitter\": None, # hsv color jitter to apply\n",
    "    \"hsv\": False, # input img color space\n",
    "    \"cmyk\": False,# input img color space\n",
    "\n",
    "    # Model Parameters\n",
    "    \"encoder\": \"se_resnet18\", # se_resnet18, se_resnet34, resnet18, resnet34, simclr\n",
    "    \"image_net_pretrained\": True, # load imgnet pretrained weights\n",
    "    \"load_path\": False, # if not False, location of pretrained encoder weights to load\n",
    "    \"freeze\": False, # True, False or \"part\" - encoder weights to freeze\n",
    "    \"dropout\": False,\n",
    "    \"num_layers\": None, # only for max pool models\n",
    "    \"aggregation\": \"gated_attention\", # gated_attention or simple_attention or max_pool\n",
    "    \"best_model_weights_path\": \"/data/MSc_students_accounts/sneha/temp_imgs/best_model_fold_0_5vvb5ni5.h5\",\n",
    "\n",
    "    # SSC module parameters\n",
    "    \"use_ssc\": False,\n",
    "    \"ssc_reconst_loss\": \"l2\",\n",
    "    \"ssc_num_routings\": 3, # R\n",
    "    \"ssc_lr\": 0.1, # initial lr for ssc_module\n",
    "    \"apply_ssc_scheduler\": (0.1,30), # Decay by factor of x0.1 every 30 epochs. False if not applying scheduler\n",
    "    \"ssc_num_stains\": 2, # S\n",
    "    \"ssc_num_groups\": 6, # M\n",
    "    \"ssc_group_width\": 3, # N\n",
    "    \"ssc_use_od\": True, # apply OD transformation in SSC capsule\n",
    "    \"ssc_in_channels\": 3 # num channels in input image (3=RGB/HSV, 4 = CMYK)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "492d227f-7e67-4d23-bafb-1b2db66124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration for run: {'num_epochs': 1, 'batch_size': 6, 'gpu_name': 'cuda:0', 'seed': 42, 'multi_gpu': False, 'tile_path': '/data/MSc_students_accounts/sneha/tiles_summary_5x_2.5mm_50%_adaptive_threshold.csv', 'data_dir': '/data/goldin/images_raw/nafld_liver_biopsies_sirius_red/', 'multiple_inference': False, 'bag_size': 10, 'resize': True, 'img_size': 224, 'stain_normalization': {'apply_reinhard': True, 'source_dir': '/data/MSc_students_accounts/sneha/sneha/sirius_red-master/reinhard_source.jpg'}, 'transform_color_jitter': None, 'hsv': False, 'cmyk': False, 'encoder': 'se_resnet18', 'image_net_pretrained': True, 'load_path': False, 'freeze': False, 'dropout': False, 'num_layers': None, 'aggregation': 'gated_attention', 'best_model_weights_path': '/data/MSc_students_accounts/sneha/temp_imgs/best_model_fold_0_5vvb5ni5.h5', 'use_ssc': False, 'ssc_reconst_loss': 'l2', 'ssc_num_routings': 3, 'ssc_lr': 0.1, 'apply_ssc_scheduler': (0.1, 30), 'ssc_num_stains': 2, 'ssc_num_groups': 6, 'ssc_group_width': 3, 'ssc_use_od': True, 'ssc_in_channels': 3}\n",
      "Using cuda:0\n",
      "Setting torch, numpy and random seeds to 42\n",
      "TEST DF                                            ndpi_file  mostly_tissue  stage\n",
      "0  /data/goldin/images_raw/nafld_liver_biopsies_s...             73      0\n",
      "1  /data/goldin/images_raw/nafld_liver_biopsies_s...             39      1\n",
      "Created Normal datasets: Testing on 2 WSIs\n",
      "Created Dataloader with num workers 16 and pinned memory False\n",
      "Getting non-SSC model\n",
      "Found 102 out of 138 matching weights. \n",
      "No matching weights for {'layer3.0.downsample.1.num_batches_tracked', 'layer3.0.se.excitation.2.weight', 'layer2.1.se.excitation.2.weight', 'layer2.0.downsample.1.num_batches_tracked', 'layer3.1.bn1.num_batches_tracked', 'layer1.1.se.excitation.2.weight', 'layer3.1.se.excitation.0.weight', 'layer2.0.se.excitation.2.weight', 'layer4.1.bn2.num_batches_tracked', 'layer1.1.se.excitation.0.weight', 'layer2.0.se.excitation.0.weight', 'layer3.0.se.excitation.0.weight', 'layer3.0.bn1.num_batches_tracked', 'layer4.0.se.excitation.2.weight', 'layer1.0.bn2.num_batches_tracked', 'layer4.1.bn1.num_batches_tracked', 'layer2.0.bn2.num_batches_tracked', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.bn2.num_batches_tracked', 'bn1.num_batches_tracked', 'layer1.0.se.excitation.0.weight', 'layer2.1.se.excitation.0.weight', 'layer1.1.bn1.num_batches_tracked', 'layer1.0.bn1.num_batches_tracked', 'layer4.0.se.excitation.0.weight', 'layer4.0.bn2.num_batches_tracked', 'layer4.1.se.excitation.2.weight', 'layer3.1.se.excitation.2.weight', 'layer4.0.bn1.num_batches_tracked', 'layer2.1.bn1.num_batches_tracked', 'layer2.0.bn1.num_batches_tracked', 'layer4.0.downsample.1.num_batches_tracked', 'layer3.1.bn2.num_batches_tracked', 'layer4.1.se.excitation.0.weight', 'layer1.1.bn2.num_batches_tracked', 'layer1.0.se.excitation.2.weight'} \n",
      "Successfully loaded model weights\n",
      "Using 1 GPU only:  cuda:0\n",
      "GatedAttention(\n",
      "  (f): Encoder(\n",
      "    (f): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Sequential(\n",
      "        (0): SEBasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SEBasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): SEBasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=8, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=8, out_features=128, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SEBasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=8, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=8, out_features=128, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): SEBasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=16, out_features=256, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SEBasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=16, out_features=256, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): SEBasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SEBasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (se): SE_Block(\n",
      "            (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
      "            (excitation): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "              (3): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (attention_a): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_b): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Loading best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WSI: [0/1]: : 1it [00:02,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 0], device='cuda:0')]\n",
      "[tensor([0, 0], device='cuda:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/data/goldin/images_raw/nafld_liver_biopsies_sirius_red/MS13-17926.SR.ndpi',\n",
       "  0),\n",
       " ('/data/goldin/images_raw/nafld_liver_biopsies_sirius_red/MS15-19086.SR.ndpi',\n",
       "  0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def run_inference(configuration):\n",
    "    print(\"Configuration for run:\", configuration)\n",
    "    # set random seed and device CPU / GPU\n",
    "    device = set_device_and_seed(GPU=True, seed=configuration[\"seed\"],\n",
    "                                 gpu_name=configuration[\"gpu_name\"])\n",
    "    # info to load the data\n",
    "    tiles_summary_data = configuration[\"tile_path\"]\n",
    "    \n",
    "    data_dir = configuration[\"data_dir\"]\n",
    "    num_workers =16\n",
    "    pin_memory=False\n",
    "    resize=configuration[\"resize\"]\n",
    "    img_size = configuration[\"img_size\"]\n",
    "\n",
    "    # make dummy labels for dataloading\n",
    "    test_df = pd.read_csv(os.path.abspath(tiles_summary_data)).dropna(subset=[\"ndpi_file\", \"mostly_tissue\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_df[\"stage\"] = test_df.index\n",
    "#     test_df = load_dataframe(os.path.abspath(tiles_summary_data), label_map)\n",
    "    print(\"TEST DF\", test_df[[\"ndpi_file\", \"mostly_tissue\",\"stage\"]])\n",
    "    # remove for experiment running\n",
    "    \n",
    "    # get normalizer\n",
    "    if configuration[\"stain_normalization\"][\"apply_reinhard\"]:\n",
    "        source_dir = configuration[\"stain_normalization\"][\"source_dir\"]\n",
    "        normalizer = Normalizer(source_path = source_dir)\n",
    "    else:\n",
    "        normalizer = None\n",
    "\n",
    "\n",
    "    # cv_results\n",
    "    test_results = {\"test_f1\": [], \"test_accuracy\": [], \"test_cm\": []}\n",
    "\n",
    "    # create datasets\n",
    "    if configuration[\"multiple_inference\"]:\n",
    "        test_dataset = MultipleInferenceDataset(test_df, data_dir,  keep_top=configuration[\"bag_size\"], verbose=False, hsv=configuration[\"hsv\"], cmyk=configuration[\"cmyk\"], transform=None, train=False,  resize=resize, img_size=img_size,normalizer=normalizer)\n",
    "        print(f\"Created MI datasets: Testing on {len(test_dataset)} WSIs\")\n",
    "    else:\n",
    "        test_dataset = CustomDataset(test_df, data_dir,  keep_top=configuration[\"bag_size\"], verbose=False, hsv=configuration[\"hsv\"], cmyk=configuration[\"cmyk\"], transform=None, train=False,  resize=resize, img_size=img_size,normalizer=normalizer)\n",
    "        print(f\"Created Normal datasets: Testing on {len(test_dataset)} WSIs\")\n",
    "\n",
    "    # create dataloaders\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size = configuration[\"batch_size\"], shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"Created Dataloader with num workers {num_workers} and pinned memory {pin_memory}\")\n",
    "    if configuration[\"use_ssc\"]:\n",
    "        print(\"Getting SSC model\")\n",
    "        model = get_model_ssc(configuration, device)\n",
    "    else:\n",
    "        print(\"Getting non-SSC model\")\n",
    "        model = get_model(configuration, device)\n",
    "    print(model)\n",
    "\n",
    "    # track model weights\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    print(f\"Loading best model\")\n",
    "    # reload best model\n",
    "    if configuration[\"multi_gpu\"]:\n",
    "        model.module.load_state_dict(torch.load(configuration[\"best_model_weights_path\"]))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(configuration[\"best_model_weights_path\"], map_location=configuration[\"gpu_name\"]))\n",
    "\n",
    "        \n",
    "    predictions = []\n",
    "    paths = []\n",
    "    color = []\n",
    "    y = []\n",
    "    visualise=True\n",
    "    model.eval()\n",
    "    test_bar = tqdm(enumerate(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, (data, label) in test_bar:\n",
    "            test_bar.set_description('WSI: [{}/{}]'.format(i, len(test_loader)))\n",
    "            # get predictions\n",
    "            data = data.to(device)\n",
    "            if configuration[\"multiple_inference\"]:\n",
    "                print(label, len(label))\n",
    "                bag_label, path = label[0], label[1]\n",
    "                label = bag_label.to(device)\n",
    "                path = torch.tensor(list(path))\n",
    "                path = path.to(device)\n",
    "                paths.append(path)\n",
    "            else:\n",
    "                \n",
    "                path =  label.to(device)\n",
    "                # we have labelled paths as the dataframe index\n",
    "                paths.append(path)\n",
    "                label = label.to(device)\n",
    "\n",
    "            if configuration[\"use_ssc\"]:\n",
    "                outputs, reconst, normed_input, A = model(data, return_attn=True)\n",
    "            else:\n",
    "                outputs, A = model(data, return_attn=True)\n",
    "\n",
    "            pred = outputs > 0.5\n",
    "            pred = pred.long()\n",
    "            predictions.append(pred)\n",
    "            reshaped_data = data.reshape(data.shape[0]*data.shape[1], data.shape[2], data.shape[3], data.shape[4])\n",
    "            # add attention code here\n",
    "            \n",
    "            \n",
    "    if configuration[\"multiple_inference\"]:\n",
    "        # get predictions per path - gather in dataframe and compute one-wins-all aggregation\n",
    "        predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
    "        paths = torch.cat(paths, dim=0).cpu().numpy()\n",
    "        predictions_df = pd.DataFrame({\"prediction\": predictions, \"wsi_path\": paths}, columns=[\"prediction\", \"wsi_path\"])\n",
    "        wsi_preds = []\n",
    "        wsi_paths = []\n",
    "        for wsi in np.unique(predictions_df[\"wsi_path\"].tolist()):\n",
    "            wsi_df = predictions_df[predictions_df[\"wsi_path\"]==wsi]\n",
    "            wsi_prediction = wsi_df[\"prediction\"].max()\n",
    "            wsi_preds.append(wsi_prediction)\n",
    "            wsi_paths.append(wsi)\n",
    "            \n",
    "        # convert integer wsi paths back to strings\n",
    "        wsi_paths =  test_dataset.tiles_summary.iloc[paths][\"ndpi_file\"].to_list()\n",
    "        print(wsi_paths)\n",
    "        return list(zip(wsi_paths, wsi_preds))\n",
    "    \n",
    "    else:\n",
    "        print(paths, predictions , sep=\"\\n\")\n",
    "        predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
    "        paths = torch.cat(paths, dim=0).cpu().numpy()\n",
    "        \n",
    "        # convert paths back to wsi_paths\n",
    "        wsi_paths = test_dataset.tiles_summary.iloc[paths][\"ndpi_file\"].to_list()\n",
    "        return list(zip(wsi_paths, predictions))\n",
    "        \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "run_inference(configuration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
